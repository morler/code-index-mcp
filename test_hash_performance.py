#!/usr/bin/env python3
"""
å“ˆå¸Œç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯• - Linusé£æ ¼ç›´æ¥æµ‹é‡

å¯¹æ¯”:
1. MD5 vs xxhash3 æ€§èƒ½
2. æ–‡ä»¶å¤§å°å¯¹æ€§èƒ½çš„å½±å“
3. ç¼“å­˜å‘½ä¸­ç‡æ”¹è¿›
"""

import time
import tempfile
import hashlib
import xxhash
from pathlib import Path
from typing import List, Tuple


def create_test_files_various_sizes(base_dir: str) -> List[Tuple[str, int]]:
    """åˆ›å»ºä¸åŒå¤§å°çš„æµ‹è¯•æ–‡ä»¶"""
    files = []
    base_path = Path(base_dir)
    
    sizes = [
        (100, "tiny"),         # 100 å­—èŠ‚
        (1024, "small"),       # 1KB 
        (10240, "medium"),     # 10KB
        (51200, "large"),      # 50KB (è§¦å‘åˆ†æ®µé‡‡æ ·)
        (512000, "xlarge"),    # 500KB
        (5120000, "xxlarge")   # 5MB (å¤§æ–‡ä»¶)
    ]
    
    for size, name in sizes:
        for i in range(10):  # æ¯ç§å¤§å°åˆ›å»º10ä¸ªæ–‡ä»¶
            file_path = base_path / f"{name}_file_{i}.py"
            
            # åˆ›å»ºæŒ‡å®šå¤§å°çš„å†…å®¹
            content = f"# æµ‹è¯•æ–‡ä»¶ {name} {i}\n"
            content += "x" * (size - len(content.encode()))
            
            file_path.write_text(content, encoding='utf-8')
            files.append((str(file_path), size))
    
    return files


def benchmark_md5(file_path: str, size: int) -> float:
    """MD5 å“ˆå¸Œæ€§èƒ½æµ‹è¯• - æ¨¡æ‹Ÿæ—§ç®—æ³•"""
    try:
        path = Path(file_path)
        stat = path.stat()
        
        start_time = time.perf_counter()
        
        # å°æ–‡ä»¶ç®€å•å¤„ç†
        if size < 1024:
            result = hashlib.md5(f"{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()
        else:
            # å¤§æ–‡ä»¶å¤„ç†
            hasher = hashlib.md5()
            hasher.update(f"{stat.st_mtime}:{stat.st_size}".encode())
            
            with open(file_path, 'rb') as f:
                hasher.update(f.read(512))
                if size > 50000:
                    f.seek(size // 2)
                    hasher.update(f.read(512))
                    f.seek(-512, 2)
                    hasher.update(f.read(512))
            
            result = hasher.hexdigest()
        
        end_time = time.perf_counter()
        return end_time - start_time
        
    except Exception:
        return 0.0


def benchmark_xxhash3(file_path: str, size: int) -> float:
    """xxhash3 æ€§èƒ½æµ‹è¯• - æ–°ä¼˜åŒ–ç®—æ³•"""
    try:
        path = Path(file_path)
        stat = path.stat()
        
        start_time = time.perf_counter()
        
        # å°æ–‡ä»¶æé€Ÿå¤„ç†
        if size < 1024:
            result = xxhash.xxh3_64(f"{stat.st_mtime}:{stat.st_size}".encode()).hexdigest()
        else:
            # å¤§æ–‡ä»¶åˆ†æ®µé‡‡æ ·
            hasher = xxhash.xxh3_64()
            hasher.update(f"{stat.st_mtime}:{stat.st_size}".encode())
            
            with open(file_path, 'rb') as f:
                hasher.update(f.read(512))
                if size > 50000:
                    f.seek(size // 2)
                    hasher.update(f.read(512))
                    f.seek(-512, 2)
                    hasher.update(f.read(512))
            
            result = hasher.hexdigest()
        
        end_time = time.perf_counter()
        return end_time - start_time
        
    except Exception:
        return 0.0


def test_hash_performance():
    """å“ˆå¸Œç®—æ³•æ€§èƒ½å¯¹æ¯”"""
    print("ğŸ å“ˆå¸Œç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    with tempfile.TemporaryDirectory() as temp_dir:
        print("ğŸ“ åˆ›å»ºæµ‹è¯•æ–‡ä»¶...")
        test_files = create_test_files_various_sizes(temp_dir)
        print(f"   åˆ›å»ºäº† {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        
        # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„æµ‹è¯•
        size_groups = {}
        for file_path, size in test_files:
            if size not in size_groups:
                size_groups[size] = []
            size_groups[size].append((file_path, size))
        
        print("\nğŸ“Š æ€§èƒ½æµ‹è¯•ç»“æœ:")
        print(f"{'æ–‡ä»¶å¤§å°':<12} {'MD5 (ms)':<12} {'xxhash3 (ms)':<14} {'æå‡å€æ•°':<10}")
        print("-" * 60)
        
        total_md5_time = 0
        total_xxhash_time = 0
        
        for size in sorted(size_groups.keys()):
            files = size_groups[size]
            
            # æµ‹è¯• MD5
            md5_times = []
            for file_path, file_size in files:
                md5_time = benchmark_md5(file_path, file_size)
                md5_times.append(md5_time)
            
            # æµ‹è¯• xxhash3
            xxhash_times = []
            for file_path, file_size in files:
                xxhash_time = benchmark_xxhash3(file_path, file_size)
                xxhash_times.append(xxhash_time)
            
            # è®¡ç®—å¹³å‡æ—¶é—´
            avg_md5 = sum(md5_times) / len(md5_times) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            avg_xxhash = sum(xxhash_times) / len(xxhash_times) * 1000
            
            total_md5_time += avg_md5
            total_xxhash_time += avg_xxhash
            
            # è®¡ç®—æå‡å€æ•°
            improvement = avg_md5 / avg_xxhash if avg_xxhash > 0 else 0
            
            # æ ¼å¼åŒ–å¤§å°æ˜¾ç¤º
            if size < 1024:
                size_str = f"{size}B"
            elif size < 1024*1024:
                size_str = f"{size//1024}KB"
            else:
                size_str = f"{size//(1024*1024)}MB"
            
            print(f"{size_str:<12} {avg_md5:<12.3f} {avg_xxhash:<14.3f} {improvement:<10.1f}x")
        
        print("-" * 60)
        total_improvement = total_md5_time / total_xxhash_time if total_xxhash_time > 0 else 0
        print(f"{'æ€»è®¡':<12} {total_md5_time:<12.1f} {total_xxhash_time:<14.1f} {total_improvement:<10.1f}x")
        
        return total_improvement


def test_cache_integration():
    """æµ‹è¯•æ–°å“ˆå¸Œç®—æ³•åœ¨ç¼“å­˜ä¸­çš„é›†æˆæ•ˆæœ"""
    print("\nğŸ”— ç¼“å­˜é›†æˆæ•ˆæœæµ‹è¯•")
    print("=" * 60)
    
    from src.core.cache import OptimizedFileCache
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
        test_files = create_test_files_various_sizes(temp_dir)
        cache = OptimizedFileCache(max_size=100, max_memory_mb=10)
        
        print("â±ï¸  ç¬¬ä¸€æ¬¡è®¿é—® (å†·ç¼“å­˜ + å“ˆå¸Œè®¡ç®—)...")
        start_time = time.perf_counter()
        for file_path, size in test_files:
            lines = cache.get_file_lines(file_path)
        first_access_time = time.perf_counter() - start_time
        
        print("âš¡ ç¬¬äºŒæ¬¡è®¿é—® (çƒ­ç¼“å­˜ï¼Œæ— å“ˆå¸Œè®¡ç®—)...")
        start_time = time.perf_counter()
        for file_path, size in test_files:
            lines = cache.get_file_lines(file_path)
        second_access_time = time.perf_counter() - start_time
        
        print("ğŸ”„ ä¿®æ”¹æ–‡ä»¶åè®¿é—® (å“ˆå¸Œæ£€æµ‹å˜æ›´)...")
        # ä¿®æ”¹ä¸€ä¸ªæ–‡ä»¶
        Path(test_files[0][0]).write_text("# ä¿®æ”¹å†…å®¹\nnew_content = True\n")
        
        start_time = time.perf_counter()
        lines = cache.get_file_lines(test_files[0][0])
        change_detect_time = time.perf_counter() - start_time
        
        cache_stats = cache.get_cache_stats()
        
        print(f"\nğŸ“ˆ ç»“æœ:")
        print(f"   å†·ç¼“å­˜æ—¶é—´: {first_access_time*1000:.1f}ms")
        print(f"   çƒ­ç¼“å­˜æ—¶é—´: {second_access_time*1000:.1f}ms") 
        print(f"   å˜æ›´æ£€æµ‹æ—¶é—´: {change_detect_time*1000:.1f}ms")
        print(f"   ç¼“å­˜æ–‡ä»¶æ•°: {cache_stats['file_count']}")
        print(f"   å†…å­˜ä½¿ç”¨: {cache_stats['memory_usage_mb']:.2f}MB")
        
        cache_speedup = first_access_time / second_access_time
        print(f"   ç¼“å­˜åŠ é€Ÿæ¯”: {cache_speedup:.1f}x")
        
        return cache_speedup


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª xxhash3 vs MD5 æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("LinusåŸåˆ™: ç”¨çœŸå®æ•°æ®æµ‹è¯•çœŸå®æ€§èƒ½")
    print("=" * 60)
    
    try:
        # å“ˆå¸Œç®—æ³•æ€§èƒ½å¯¹æ¯”
        hash_improvement = test_hash_performance()
        
        # ç¼“å­˜é›†æˆæµ‹è¯•
        cache_speedup = test_cache_integration()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ“Š å“ˆå¸Œç®—æ³•æå‡: {hash_improvement:.1f}x")
        print(f"ğŸš€ ç¼“å­˜ç³»ç»ŸåŠ é€Ÿ: {cache_speedup:.1f}x")
        
        if hash_improvement > 2.0:
            print("âœ… xxhash3 æ˜¾è‘—ä¼˜äº MD5 - å‡çº§æˆåŠŸ!")
        else:
            print("âš ï¸  æ€§èƒ½æå‡æœ‰é™ï¼Œä½†ä»ç„¶æ˜¯æ”¹è¿›")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)